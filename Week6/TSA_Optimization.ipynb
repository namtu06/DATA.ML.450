{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0076e4c",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1bde7",
   "metadata": {},
   "source": [
    "Increasing the range of all parameters for iterations and the max resources to 100 seem to yield a better score although very marginally, 0.97333 to 0.98.\n",
    "Perhaps increasing the amount of resources into the thousands will improve the score further although that will consume a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3686bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_samples_split': 3, 'n_estimators': 9}\n",
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "\n",
    "#Original code\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "np.random.seed(0)\n",
    "\n",
    "param_distributions = {\"max_depth\": [3, None],\n",
    "                       \"min_samples_split\": randint(2, 11)}\n",
    "\n",
    "\n",
    "search = HalvingRandomSearchCV(clf, param_distributions,\n",
    "                               resource='n_estimators',\n",
    "                               max_resources=10,\n",
    "                               random_state=0).fit(X, y)\n",
    "print(search.best_params_)\n",
    "\n",
    "print(search.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363595f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 11, 'min_samples_split': 10, 'n_estimators': 81}\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "#Altered parameters\n",
    "\n",
    "param_distributions = {\"max_depth\": [11,13,15,17,19,21,23,25],\n",
    "                       \"min_samples_split\": randint(2, 60)}\n",
    "\n",
    "\n",
    "search = HalvingRandomSearchCV(clf, param_distributions,\n",
    "                               resource='n_estimators',\n",
    "                               max_resources=100,\n",
    "                               random_state=0).fit(X, y)\n",
    "print(search.best_params_)\n",
    "\n",
    "print(search.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e85b6",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a744b",
   "metadata": {},
   "source": [
    "After much attempt in trying to improve the score by changing the various range of estimator numbers, learning rate and number of iterations, the best results seem to remain the one provided by SVC (the best result from Adaboost classifier is the exact same as the SVC). Not only did the Adaboost take significantly longer, the best result possible is the same as SVC classifier's result; any different result are always lower. So, the SVC classifer is highly optimized and very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c3df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC classifier results: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data= load_iris()\n",
    "X=data.data\n",
    "y=data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.75,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "        'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "        'degree': Integer(1,8),\n",
    "        'kernel': Categorical(['linear', 'poly', 'rbf']),\n",
    "    },\n",
    "    n_iter=32,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# executes bayesian optimization\n",
    "_ = opt.fit(X_train, y_train)\n",
    "\n",
    "# model can be saved, used for predictions or scoring\n",
    "print(\"SVC classifier results:\",opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f915b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost classifier results: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "opt = BayesSearchCV(\n",
    "    AdaBoostClassifier(),\n",
    "    {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'learning_rate': Real(0.01, 1.0,prior='log-uniform'),\n",
    "\n",
    "    },\n",
    "    n_iter=100,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# executes bayesian optimization\n",
    "_ = opt.fit(X_train, y_train)\n",
    "\n",
    "# model can be saved, used for predictions or scoring\n",
    "print(\"Adaboost classifier results:\",opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4228819",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cbd2b",
   "metadata": {},
   "source": [
    "Reduced the number of epochs and trials in order to save time, we can still compare the functions since they're all running the same parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbadb48",
   "metadata": {},
   "source": [
    "Results: CELU > ReLU > Tanh > Sigmoid.\n",
    "There seems to be a consistent approximately 0.03 difference between one activation and its neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43743915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running study for ReLU\n",
      "Best accuracy for ReLU: 0.7688\n",
      "\n",
      "Running study for Tanh\n",
      "Best accuracy for Tanh: 0.7359\n",
      "\n",
      "Running study for CELU\n",
      "Best accuracy for CELU: 0.8031\n",
      "\n",
      "Running study for Sigmoid\n",
      "Best accuracy for Sigmoid: 0.6969\n",
      "Activation comparison\n",
      "ReLU    : 0.7688\n",
      "Tanh    : 0.7359\n",
      "CELU    : 0.8031\n",
      "Sigmoid : 0.6969\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 10\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 5\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 10\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 5\n",
    "ACTIVATIONS = {\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"Tanh\": nn.Tanh,\n",
    "    \"CELU\": nn.CELU,\n",
    "    \"Sigmoid\": nn.Sigmoid,\n",
    "}\n",
    "\n",
    "\n",
    "def define_model(trial, activation_cls):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 28 * 28\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(f\"n_units_l{i}\", 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(activation_cls())   \n",
    "\n",
    "        p = trial.suggest_float(f\"dropout_l{i}\", 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def get_mnist():\n",
    "    # Load FashionMNIST dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(DIR, train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "def objective(trial, activation_cls):\n",
    "    model = define_model(trial, activation_cls).to(DEVICE)\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader, valid_loader = get_mnist()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            data = data.view(data.size(0), -1).to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                    break\n",
    "\n",
    "                data = data.view(data.size(0), -1).to(DEVICE)\n",
    "                target = target.to(DEVICE)\n",
    "\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #optuna.logging.set_verbosity(optuna.logging.ERROR) #Silence progress output to not fill up the screen\n",
    "    results = {}\n",
    "\n",
    "    for name, activation_cls in ACTIVATIONS.items():\n",
    "        print(f\"\\nRunning study for {name}\")\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, activation_cls),\n",
    "            n_trials=30,\n",
    "            timeout=600,\n",
    "        )\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "        results[name] = best_trial.value\n",
    "\n",
    "        print(f\"Best accuracy for {name}: {best_trial.value:.4f}\")\n",
    "\n",
    "    print(\"Activation comparison\")\n",
    "    for act, acc in results.items():\n",
    "        print(f\"{act:8s}: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
